# Vue d'ensemble

<CourseFloatingBanner
    unit={4}
    classNames="absolute z-10 right-0 top-0"
/>

Dans cette unit√©, nous examinerons certaines des nombreuses am√©liorations et extensions des mod√®les de diffusion apparaissant dans les recherches les plus r√©centes. Elle sera moins ax√©e sur le code que les unit√©s pr√©c√©dentes et est con√ßue pour vous donner un point de d√©part pour des recherches plus approfondies.

## Vue d'ensemble de cette unit√© :rocket:

Les diff√©rentes √©tapes √† suivre pour cette unit√© :  
- Lisez le mat√©riel ci-dessous pour avoir une vue d'ensemble des id√©es cl√©s de cette unit√©
- Approfondissez les sujets sp√©cifiques gr√¢ce aux vid√©os et aux ressources associ√©es.
- Explorez les *notebooks* de d√©monstration, puis lisez la section " Et ensuite ?" pour obtenir des suggestions de projets.


## √âchantillonnage plus rapide par distillation

La distillation progressive est une technique permettant de prendre un mod√®le de diffusion existant et de l'utiliser pour entra√Æner une nouvelle version du mod√®le qui n√©cessite moins d'√©tapes pour l'inf√©rence. Le mod√®le "√©l√®ve" est initialis√© √† partir des poids du mod√®le "enseignant". Pendant l'entra√Ænement, le mod√®le enseignant effectue deux √©tapes d'√©chantillonnage et le mod√®le de √©tudiant tente de faire correspondre la pr√©diction r√©sultante en une seule √©tape. Ce processus peut √™tre r√©p√©t√© plusieurs fois, le mod√®le √©tudiant de l'it√©ration pr√©c√©dente devenant le mod√®le enseignant pour l'√©tape suivante. Le r√©sultat est un mod√®le qui peut produire des √©chantillons d√©cents en beaucoup moins d'√©tapes (g√©n√©ralement 4 ou 8) que le mod√®le enseignant d'origine. Le m√©canisme de base est illustr√© dans ce diagramme tir√© de [l'article qui a introduit l'id√©e](http://arxiv.org/abs/2202.00512) :

![image](https://user-images.githubusercontent.com/6575163/211016659-7dac24a5-37e2-45f9-aba8-0c573937e7fb.png)

_Illustration de la distillation progressive issue de ce [papier](http://arxiv.org/abs/2202.00512))_

L'id√©e d'utiliser un mod√®le existant pour "enseigner" un nouveau mod√®le peut √™tre √©tendue pour cr√©er des mod√®les guid√©s dans lesquels la technique de guidage sans classifieur est utilis√©e par le mod√®le enseignant et le mod√®le √©tudiant doit apprendre √† produire un r√©sultat √©quivalent en une seule √©tape sur la base d'une entr√©e suppl√©mentaire sp√©cifiant l'√©chelle de guidage cibl√©e. Cela permet de r√©duire encore le nombre d'√©valuations de mod√®les n√©cessaires pour produire des √©chantillons de haute qualit√©. [Cette vid√©o ](https://www.youtube.com/watch?v=ZXuK6IRJlnk) (en anglais) donne un aper√ßu de l'approche.

R√©f√©rences principales :  
- [Progressive Distillation For Fast Sampling Of Diffusion Models](http://arxiv.org/abs/2202.00512)
- [On Distillation Of Guided Diffusion Models](http://arxiv.org/abs/2210.03142)

## Am√©lioration de l'entra√Ænement

Plusieurs astuces suppl√©mentaires ont √©t√© mises au point pour am√©liorer l'entra√Ænement des mod√®les de diffusion. Dans cette section, nous avons essay√© de pr√©senter les id√©es principales des articles r√©cents. Il y a un flux constant de recherches qui sortent avec des am√©liorations suppl√©mentaires, donc si vous voyez un article qui devrait √™tre ajout√© ici, veuillez nous le faire savoir !

![image](https://user-images.githubusercontent.com/6575163/211021220-e87ca296-cf15-4262-9359-7aeffeecbaae.png)
_Figure 2 du papier [ERNIE-ViLG 2.0](http://arxiv.org/abs/2210.15257)_

Am√©liorations principales de l'entra√Ænement :
- R√©glage du planificateur du bruit, de la pond√©ration de la perte et des trajectoires d'√©chantillonnage pour un entra√Ænement plus efficace. Un excellent papier explorant certains de ces choix de conception est [*Elucidating the Design Space of Diffusion-Based Generative Models*] (http://arxiv.org/abs/2206.00364) par Karras et al.
- Entra√Ænement sur divers rapports d'aspect, comme d√©crit dans [cette vid√©o du lancement du cours] (https://www.youtube.com/watch?v=g6tIUrMvOec) (en anglais).
- Mod√®les de diffusion en cascade, entra√Ænant un mod√®le √† basse r√©solution, puis un ou plusieurs mod√®les en super-r√©solution. Utilis√©s dans DALLE-2, Imagen et d'autres pour la g√©n√©ration d'images √† haute r√©solution.
- Meilleur conditionnement, incorporation d'ench√¢ssement textuels riches ([Imagen](https://arxiv.org/abs/2205.11487) utilise un grand mod√®le de langage appel√© T5) ou plusieurs types de conditionnement ([eDiffi](http://arxiv.org/abs/2211.01324)).
- "Am√©lioration des connaissances" : incorporation de mod√®les de sous-titrage d'images et de d√©tection d'objets pr√©-entra√Æn√©s dans le processus d'entra√Ænement afin de cr√©er des sous-titres plus informatifs et d'obtenir de meilleures performances ([ERNIE-ViLG 2.0](http://arxiv.org/abs/2210.15257)).
- "M√©lange d'experts de d√©bruitage" (MoDE) : entra√Æner diff√©rentes variantes du mod√®le ("experts") pour diff√©rents niveaux de bruit, comme illustr√© dans l'image ci-dessus tir√©e du papier [ERNIE-ViLG 2.0] (http://arxiv.org/abs/2210.15257).

R√©f√©rences principales :  
- [Elucidating the Design Space of Diffusion-Based Generative Models](http://arxiv.org/abs/2206.00364)
- [eDiffi: Text-to-Image Diffusion Models with an Ensemble of Expert Denoisers](http://arxiv.org/abs/2211.01324)
- [ERNIE-ViLG 2.0: Improving Text-to-Image Diffusion Model with Knowledge-Enhanced Mixture-of-Denoising-Experts](http://arxiv.org/abs/2210.15257)
- [Imagen - Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding](https://arxiv.org/abs/2205.11487) ([site d√©mo](https://imagen.research.google/))

## Plus de contr√¥le pour la g√©n√©ration et l'√©dition

Outre les am√©liorations apport√©es √† l'entra√Ænement, plusieurs innovations ont √©t√© apport√©es √† la phase d'√©chantillonnage et d'inf√©rence, y compris de nombreuses approches qui peuvent ajouter de nouvelles capacit√©s aux mod√®les de diffusion existants.

![image](https://user-images.githubusercontent.com/6575163/212529129-3de41cf4-6f70-4607-8448-e9bbe9d190cf.png)
_√âchantillons g√©n√©r√©s par [eDiffi](http://arxiv.org/abs/2211.01324)_

La vid√©o ['Editing Images with Diffusion Models'] (https://www.youtube.com/watch?v=zcG7tG3xS3s) (en anglais) donne un aper√ßu des diff√©rentes m√©thodes utilis√©es pour √©diter des images existantes avec des mod√®les de diffusion. Les techniques disponibles peuvent √™tre divis√©es en quatre cat√©gories principales :

1) Ajouter du bruit, puis d√©bruiter avec un nouveau prompt. C'est l'id√©e qui sous-tend le pipeline `img2img`, qui a √©t√© modifi√© et √©tendu dans plusieurs articles :
- [SDEdit](https://sde-image-editing.github.io/) et [MagicMix](https://magicmix.github.io/) s'inspirent de cette id√©e
- [DDIM inversion](https://arxiv.org/abs/2010.02502) utilise le mod√®le pour "inverser" la trajectoire d'√©chantillonnage plut√¥t que d'ajouter un bruit al√©atoire, ce qui permet un meilleur contr√¥le
- [Null-text Inversion](https://null-text-inversion.github.io/) am√©liore consid√©rablement les performances de ce type d'approche en optimisant √† chaque √©tape les ench√¢ssements de texte inconditionnels utilis√©s pour le guidage sans classifieur, ce qui permet d'obtenir une √©dition d'images textuelles de tr√®s haute qualit√©.
2) Extension des id√©es du point (1), mais avec un masque permettant de contr√¥ler l'endroit o√π l'effet est appliqu√©
- [Blended Diffusion](https://omriavrahami.com/blended-diffusion-page/) introduit l'id√©e de base
- [Cette d√©mo](https://huggingface.co/spaces/nielsr/text-based-inpainting) utilise un mod√®le de segmentation existant (CLIPSeg) pour cr√©er le masque sur la base d'une description textuelle
- [DiffEdit](https://arxiv.org/abs/2210.11427) est un excellent papier montrant comment le mod√®le de diffusion lui-m√™me peut √™tre utilis√© pour g√©n√©rer un masque appropri√© pour l'√©dition de l'image en fonction du texte
- [SmartBrush: Text and Shape Guided Object Inpainting with Diffusion Model](https://arxiv.org/abs/2212.05034) *finetune* un mod√®le de diffusion pour une peinture guid√©e par un masque
3) Contr√¥le de l'attention crois√©e : utilisation du m√©canisme d'attention crois√©e dans les mod√®les de diffusion pour contr√¥ler l'emplacement spatial des modifications afin d'exercer un contr√¥le plus fin
- [Prompt-to-Prompt Image Editing with Cross Attention Control](https://arxiv.org/abs/2208.01626) est l'article cl√© qui a introduit cette id√©e, et la technique a [depuis √©t√© appliqu√©e √† Stable Diffusion](https://wandb.ai/wandb/cross-attention-control/reports/Improving-Generative-Images-with-Instructions-Prompt-to-Prompt-Image-Editing-with-Cross-Attention-Control--VmlldzoyNjk2MDAy)
- TCette id√©e est √©galement utilis√©e pour 'paint-with-words' ([eDiffi](http://arxiv.org/abs/2211.01324), voir ci-dessus)
4) Finetuner (surapprendre) sur une seule image, puis g√©n√©rer avec le mod√®le *finetun√©*. Les articles suivants ont tous deux publi√© des variantes de cette id√©e √† peu pr√®s au m√™me moment :
- [Imagic: Text-Based Real Image Editing with Diffusion Models](https://arxiv.org/abs/2210.09276)
- [UniTune: Text-Driven Image Editing by Fine Tuning an Image Generation Model on a Single Image](https://arxiv.org/abs/2210.09477)

Le papier [InstructPix2Pix : Learning to Follow Image Editing Instructions](https://arxiv.org/abs/2211.09800) est remarquable en ce sens qu'il utilise certaines des techniques d'√©dition d'images d√©crites ci-dessus pour construire un jeu de donn√©es synth√©tique de paires d'images accompagn√©es d'instructions d'√©dition d'images (g√©n√©r√©es avec GPT3.5) afin d'entra√Æner un nouveau mod√®le capable d'√©diter des images sur la base d'instructions en langage naturel.


## Video

![image](https://user-images.githubusercontent.com/6575163/213657523-be40178a-4357-410b-89e3-a4cbd8528900.png)
_Images fixes d'[exemples de vid√©os g√©n√©r√©es avec Imagen Video](https://imagen.research.google/video/)_

Une vid√©o peut √™tre repr√©sent√©e comme une s√©quence d'images, et les id√©es fondamentales des mod√®les de diffusion peuvent √™tre appliqu√©es √† ces s√©quences. Les travaux r√©cents se sont concentr√©s sur la recherche d'architectures appropri√©es (telles que les "3D UNets" qui op√®rent sur des s√©quences enti√®res) et sur l'utilisation efficace des donn√©es vid√©o. √âtant donn√© que les vid√©os √† haute fr√©quence d'images comportent beaucoup plus de donn√©es que les images fixes, les approches actuelles tendent √† g√©n√©rer d'abord des vid√©os √† faible r√©solution et √† faible fr√©quence d'images, puis √† appliquer la super-r√©solution spatiale et temporelle pour produire les sorties vid√©o finales de haute qualit√©.

R√©f√©rences principales :  
- [Video Diffusion Models](https://video-diffusion.github.io/)
- [Imagen Video: High Definition Video Generation With Diffusion Models](https://imagen.research.google/video/paper.pdf)

## Audio

![image](https://user-images.githubusercontent.com/6575163/213657272-a1b54017-216f-453b-9b28-97c6fef21f54.png)

_Un spectrogramme g√©n√©r√© avec [Riffusion](https://www.riffusion.com/about)_

Bien que des travaux aient √©t√© r√©alis√©s pour g√©n√©rer du son directement √† l'aide de mod√®les de diffusion (par exemple [DiffWave](https://arxiv.org/abs/2009.09761)), l'approche la plus fructueuse jusqu'√† pr√©sent a consist√© √† convertir le signal audio en ce que l'on appelle un spectrogramme, qui "encode" effectivement le son sous la forme d'une "image" en 2D qui peut ensuite √™tre utilis√©e pour entra√Æner les mod√®les de diffusion que nous avons l'habitude d'utiliser pour la g√©n√©ration d'images. Les spectrogrammes ainsi g√©n√©r√©s peuvent ensuite √™tre convertis en donn√©es audio √† l'aide des m√©thodes existantes. Cette approche est √† l'origine de Riffusion, qui a r√©cemment √©t√© publi√© et a permis de *finetuner* Stable Diffusion pour g√©n√©rer des spectrogrammes conditionn√©s par le texte. [Essayez-le ici](https://www.riffusion.com/).

Le domaine de la g√©n√©ration d'audio √©volue tr√®s rapidement. Au cours de la semaine derni√®re (√† l'heure o√π nous √©crivons ces lignes), au moins cinq nouvelles avanc√©es ont √©t√© annonc√©es, qui sont marqu√©es d'une √©toile dans la liste ci-dessous :

R√©f√©rences principales :  
- [DiffWave: A Versatile Diffusion Model for Audio Synthesis](https://arxiv.org/abs/2009.09761)
- [Riffusion](https://www.riffusion.com/about) (et son [code](https://github.com/riffusion/riffusion))
- *[MusicLM](https://google-research.github.io/seanet/musiclm/examples/) de Google g√©n√®re un son coh√©rent √† partir d'un texte et peut √™tre conditionn√© avec des m√©lodies fredonn√©es ou siffl√©es.
- *[RAVE2](https://github.com/acids-ircam/RAVE), une nouvelle version d'un auto-encodeur variationnel qui sera utile pour la diffusion latente dans les t√¢ches audio. Il est utilis√© dans le mod√®le *[AudioLDM](https://twitter.com/LiuHaohe/status/1619119637660327936?s=20&t=jMkPWBFuAH19HI9m5Sklmg).
- *[Noise2Music](https://noise2music.github.io/), un mod√®le de diffusion entra√Æn√© √† produire des clips audio de 30 secondes en haute qualit√© sur la base de descriptions textuelles.
- *[Make-An-Audio: Text-To-Audio Generation with Prompt-Enhanced Diffusion Models](https://text-to-audio.github.io/), un mod√®le de diffusion entra√Æn√© √† g√©n√©rer divers sons √† partir d'un texte
- *[Mo√ªsai: Text-to-Music Generation with Long-Context Latent Diffusion](https://arxiv.org/abs/2301.11757)

## Nouvelles architectures et approches : vers un "raffinement it√©ratif"

![image](https://user-images.githubusercontent.com/6575163/213731066-0fbe38a7-233f-42be-99fc-38cea889c86b.png)

_Figure 1 du papier [Cold Diffusion](http://arxiv.org/abs/2208.09392)_

Nous d√©passons peu √† peu la d√©finition √©troite initiale des mod√®les de "diffusion" pour nous orienter vers une classe plus g√©n√©rale de mod√®les qui effectuent un **raffinement it√©ratif**, o√π une certaine forme de corruption (comme l'ajout d'un bruit gaussien dans le processus de diffusion vers l'avant) est progressivement invers√©e pour g√©n√©rer des √©chantillons. L'article "*Cold Diffusion*" a d√©montr√© que de nombreux autres types de corruption peuvent √™tre "d√©faits" de mani√®re it√©rative pour g√©n√©rer des images (exemples ci-dessus), et des approches r√©centes bas√©es sur des *transfomers* ont d√©montr√© l'efficacit√© du remplacement ou du masquage de *token* en tant que strat√©gie de bruitage.

![image](https://user-images.githubusercontent.com/6575163/213731351-7fd6c98c-6ba6-4bd9-a898-230002fc334f.png)

_Pipeline de[MaskGIT](http://arxiv.org/abs/2202.04200)_

L'architecture UNet au c≈ìur de nombreux mod√®les de diffusion actuels est √©galement remplac√©e par d'autres solutions, notamment diverses architectures bas√©es sur des *transformers*. Dans [Scalable Diffusion Models with Transformers (DiT)] (https://www.wpeebles.com/DiT), un *transformer* est utilis√© √† la place du UNet pour une approche de mod√®le de diffusion assez standard, avec d'excellents r√©sultats. [Recurrent Interface Networks](https://arxiv.org/pdf/2212.11972.pdf) applique une nouvelle architecture bas√©e sur un *transformer* et une strat√©gie d'entra√Ænement √† la recherche d'une efficacit√© accrue. [MaskGIT](http://arxiv.org/abs/2202.04200) et [MUSE](http://arxiv.org/abs/2301.00704) utilisent des *transformers* pour travailler avec des repr√©sentations d'images par *tokens*, bien que le mod√®le [Paella](https://arxiv.org/abs/2211.07292v1) d√©montre qu'un UNet peut √©galement √™tre appliqu√© avec succ√®s √† ces r√©gimes bas√©s sur des *tokens*.

Avec chaque nouveau papier, des approches plus efficaces sont d√©velopp√©es, et il faudra peut-√™tre attendre un certain temps avant de voir √† quoi ressemblent les performances maximales pour ce type de t√¢ches d'affinage it√©ratif. Il reste encore beaucoup de choses √† explorer !

R√©f√©rences principales :  
- [Cold Diffusion: Inverting Arbitrary Image Transforms Without Noise](http://arxiv.org/abs/2208.09392)
- [Scalable Diffusion Models with Transformers (DiT)](https://www.wpeebles.com/DiT)
- [MaskGIT: Masked Generative Image Transformer](http://arxiv.org/abs/2202.04200)
- [Muse: Text-To-Image Generation via Masked Generative Transformers](http://arxiv.org/abs/2301.00704)
- [Fast Text-Conditional Discrete Denoising on Vector-Quantized Latent Spaces (Paella)](https://arxiv.org/abs/2211.07292v1)
- [Recurrent Interface Networks](https://arxiv.org/pdf/2212.11972.pdf) : une nouvelle architecture prometteuse qui permet de g√©n√©rer des images √† haute r√©solution sans recourir √† la diffusion latente ou √† la super-r√©solution. Voir √©galement [simple diffusion : End-to-end diffusion for high-resolution images] (https://arxiv.org/abs/2301.11093) qui souligne l'importance du planificateur du bruit pour l'entra√Ænement √† des r√©solutions plus √©lev√©es.

## Notebooks

| Chapitre                                    | Colab                                                                                                                                                                                               | Kaggle                                                                                                                                                                                                   | Gradient                                                                                                                                                                               | Studio Lab                                                                                                                                                                                                   |
|:--------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| D√©bruitage inverse des mod√®les de diffusion implicites                              | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/diffusion-models-class/blob/main/fr/unit4/_ddim_inversion.ipynb)              | [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/huggingface/diffusion-models-class/blob/main/fr/unit4/_ddim_inversion.ipynb)              | [![Gradient](https://assets.paperspace.io/img/gradient-badge.svg)](https://console.paperspace.com/github/huggingface/diffusion-models-class/blob/main/fr/unit4/ddim_inversion.ipynb)              | [![Open In SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/diffusion-models-class/blob/main/fr/unit4/_ddim_inversion.ipynb)              |
| Diffusion pour l'audio                      | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/diffusion-models-class/blob/main/fr/unit4/_diffusion_for_audio.ipynb)              | [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/huggingface/diffusion-models-class/blob/main/fr/unit4/_diffusion_for_audio.ipynb)              | [![Gradient](https://assets.paperspace.io/img/gradient-badge.svg)](https://console.paperspace.com/github/huggingface/diffusion-models-class/blob/main/fr/unit4/_diffusion_for_audio.ipynb)              | [![Open In SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/diffusion-models-class/blob/main/fr/unit4/_diffusion_for_audio.ipynb)              |

Nous avons abord√© un grand nombre d'id√©es diff√©rentes dans cette unit√©, dont beaucoup m√©riteraient de faire l'objet de le√ßons plus d√©taill√©es √† l'avenir. Pour l'instant, vous pouvez aborder deux de ces nombreux sujets via les *notebook* que nous avons pr√©par√©s.
- **Le d√©bruitage inverse des mod√®les de diffusion implicites** montre comment une technique appel√©e inversion peut √™tre utilis√©e pour √©diter des images √† l'aide de mod√®les de diffusion existants.
- **Diffusion pour l'audio** introduit l'id√©e de spectrogrammes et montre un exemple minimal de *finetuning* d'un mod√®le de diffusion pour l'audio sur un genre de musique sp√©cifique.

## Et ensuite ?

Il s'agit de la derni√®re unit√© de ce cours actuellement, ce qui signifie que la suite ne d√©pend que de vous ! N'oubliez pas que vous pouvez toujours poser des questions et discuter de vos projets sur le [Discord] d'Hugging Face](https://huggingface.co/join/discord). Nous avons h√¢te de voir ce que vous allez cr√©er ü§ó