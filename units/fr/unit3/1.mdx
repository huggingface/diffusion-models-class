# Vue d'ensemble

<CourseFloatingBanner
    unit={3}
    classNames="absolute z-10 right-0 top-0"
/>

Dans cette unit√©, vous allez d√©couvrir un puissant mod√®le de diffusion appel√© Stable Diffusion (SD) et explorer ce qu'il peut faire.

## Vue d'ensemble de cette unit√© :rocket:

Les diff√©rentes √©tapes √† suivre pour cette unit√© :

- Lisez le mat√©riel ci-dessous pour avoir une vue d'ensemble des id√©es cl√©s de cette unit√©
- Consultez le [*notebook* **Introduction √† Stable Diffusion**_] pour voir l'application pratique de SD dans des cas d'utilisation courants.
- (Facultatif) Consultez la vid√©o [_**Stable Diffusion Deep Dive**_](https://www.youtube.com/watch?app=desktop&v=0_BBRNYInx8) (en anglais) et le [_**notebook**_](https://github.com/fastai/diffusion-nbs/blob/master/Stable%20Diffusion%20Deep%20Dive.ipynb) qui l'accompagne pour une exploration plus approfondie des diff√©rents composants et de la fa√ßon dont ils peuvent √™tre adapt√©s √† diff√©rents effets. Ce mat√©riel a √©t√© cr√©√© pour le cours de FastAI, [*Stable Diffusion from the Foundations*](https://www.fast.ai/posts/part2-2022.html) (en anglais), ce qui en fait un excellent compl√©ment √† ce cours pour tous ceux qui sont curieux de construire ce type de mod√®les √† partir de z√©ro. 

## Introduction

![SD example images](sd_demo_images.jpg)<br>
_Exemples d'images g√©n√©r√©es √† l'aide de Stable Diffusion_

Stable Diffusion est un puissant mod√®le de diffusion latent conditionn√© par le texte. Ne vous inqui√©tez pas, nous expliquerons ces mots dans quelques instants ! Sa capacit√© √† cr√©er des images √©tonnantes √† partir de descriptions textuelles en a fait une sensation sur Internet. Dans cette unit√©, nous allons explorer le fonctionnement du mod√®le de diffusion latent et voir quels sont ses autres atouts.
 
## Diffusion latente

Plus la taille des images augmente, plus la puissance de calcul n√©cessaire pour travailler avec ces images s'accro√Æt. Ceci est particuli√®rement prononc√© dans une op√©ration appel√©e auto-attention, o√π le nombre d'op√©rations cro√Æt de fa√ßon quadratique avec le nombre d'entr√©es. Une image carr√©e de 128 pixels a 4 fois plus de pixels qu'une image carr√©e de 64 pixels, et n√©cessite donc 16 fois (c'est-√†-dire 4¬≤) la m√©moire et le calcul dans une couche d'auto-attention. Ce probl√®me se pose pour tous ceux qui souhaitent g√©n√©rer des images en haute r√©solution !

![latent diffusion diagram](https://github.com/CompVis/latent-diffusion/raw/main/assets/modelfigure.png)<br>
_Diagramme provenant du papier [*High-Resolution Image Synthesis with Latent Diffusion Models*](http://arxiv.org/abs/2112.10752)_

La diffusion latente permet d'att√©nuer ce probl√®me en utilisant un mod√®le distinct appel√© auto-encodeur variationnel (VAE pour *Variational Auto-Encoder*) pour **compresser** les images dans une dimension spatiale plus petite. Le raisonnement sous-jacent est que les images ont tendance √† contenir une grande quantit√© d'informations redondantes. Avec suffisamment de donn√©es d'entra√Ænement, un VAE peut esp√©rer apprendre √† produire une repr√©sentation beaucoup plus petite d'une image d'entr√©e et ensuite reconstruire l'image sur la base de cette petite repr√©sentation **latente** avec un haut degr√© de fid√©lit√©. Le VAE utilis√© dans SD prend des images √† 3 canaux et produit une repr√©sentation latente √† 4 canaux avec un facteur de r√©duction de 8 pour chaque dimension spatiale. En d'autres termes, une image d'entr√©e carr√©e de 512 pixels sera compress√©e en une repr√©sentation latente de 4x64x64.

En appliquant le processus de diffusion √† ces **repr√©sentations latentes** plut√¥t qu'√† des images en pleine r√©solution, nous pouvons b√©n√©ficier de nombreux avantages li√©s √† l'utilisation d'images plus petites (moins d'utilisation de m√©moire, moins de couches n√©cessaires dans le UNet, des temps de g√©n√©ration plus rapides...) tout en d√©codant le r√©sultat en une image en haute r√©solution une fois que nous sommes pr√™ts √† voir le r√©sultat final. Cette solution permet de r√©duire consid√©rablement le co√ªt de l'entra√Ænement et d'ex√©cution de ces mod√®les. 

## Conditionnement

Dans l'unit√© 2, nous avons montr√© comment l'apport d'informations suppl√©mentaires au UNet nous permet d'exercer un contr√¥le suppl√©mentaire sur les types d'images g√©n√©r√©es. C'est ce que nous appelons le conditionnement. √âtant donn√© une version bruit√©e d'une image, le mod√®le est charg√© de pr√©dire la version d√©bruit√©e **en fonction d'indices suppl√©mentaires** tels qu'une √©tiquette de classe ou, dans le cas de Stable Diffusion, une description textuelle de l'image. Au moment de l'inf√©rence, nous pouvons introduire la description d'une image que nous aimerions voir et un peu de bruit pur comme point de d√©part, et le mod√®le fait de son mieux pour " d√©bruiter " l'entr√©e al√©atoire en quelque chose qui corresponde √† la l√©gende. 

![text encoder diagram](text_encoder_noborder.png)<br>
_Diagramme montrant le processus d'encodage de texte qui transforme le prompt d'entr√©e en un ensemble d'ench√¢ssements de texte (les `encoder_hidden_states`) qui peuvent ensuite √™tre introduits dans l'UNet en tant que condition._

Pour que cela fonctionne, nous devons cr√©er une repr√©sentation num√©rique du texte qui capture des informations pertinentes sur ce qu'il d√©crit. Pour ce faire, SD s'appuie sur un *transformer* pr√©-entra√Æn√© bas√© sur ce que l'on appelle CLIP. L'encodeur textuel de CLIP a √©t√© con√ßu pour traiter les l√©gendes d'images sous une forme pouvant √™tre utilis√©e pour comparer les images et le texte, il est donc bien adapt√© √† la t√¢che de cr√©ation de repr√©sentations utiles √† partir de descriptions d'images. Un prompt est d'abord *tokenizer* (sur la base d'un large vocabulaire o√π chaque mot ou sous-mot se voit attribuer un *token* sp√©cifique), puis transmis √† l'encodeur textuel de CLIP, qui produit un vecteur √† 768 dimensions (dans le cas de SD 1.X) ou √† 1024 dimensions (SD 2.X) pour chaque *tokens*. Pour que les choses restent coh√©rentes, les prompts sont toujours rembourr√©s/tronqu√©s pour avoir une longueur de 77 *tokens*, de sorte que la repr√©sentation finale que nous utilisons comme conditionnement est un tenseur de forme 77x1024 par prompt.

![conditioning diagram](sd_unet_color.png)

Alors, comment introduire ces informations de conditionnement dans l'UNet pour qu'il les utilise dans ses pr√©dictions ? La r√©ponse est ce que l'on appelle l'attention crois√©e. Des couches d'attention crois√©e sont diss√©min√©es dans l'UNet. Chaque emplacement spatial de l'UNet peut "s'int√©resser" √† diff√©rents *tokens* dans le conditionnement du texte, en apportant des informations pertinentes provenant du prompt. Le diagramme ci-dessus montre comment ce conditionnement textuel (ainsi que le conditionnement bas√© sur le temps) est fourni √† diff√©rents endroits. Comme vous pouvez le constater, √† chaque niveau, l'UNet a de nombreuses possibilit√©s d'utiliser ce conditionnement !

## Guidage sans classification

Il s'av√®re que m√™me avec tous les efforts d√©ploy√©s pour rendre le texte de conditionnement aussi utile que possible, le mod√®le a toujours tendance √† s'appuyer principalement sur l'image d'entr√©e bruyante plut√¥t que sur le prompt lorsqu'il fait ses pr√©dictions. D'une certaine mani√®re, de nombreuses l√©gendes ne sont que vaguement li√©es aux images qui leur sont associ√©es et le mod√®le apprend donc √† ne pas trop s'appuyer sur les descriptions ! Toutefois, cela n'est pas souhaitable lorsqu'il s'agit de g√©n√©rer de nouvelles images : si le mod√®le ne suit pas le prompt, nous risquons d'obtenir des images qui ne sont pas du tout li√©es √† notre description.

![CFG scale demo grid](cfg_example_0_1_2_10.jpeg)<br>
_Images g√©n√©r√©es √† partir du prompt "Une peinture √† l'huile d'un colley avec un chapeau haut de forme" avec l'√©chelle CFG 0, 1, 2 et 10 (de gauche √† droite)_

Pour r√©soudre ce probl√®me, nous utilisons une astuce appel√©e "Classifier-Free Guidance" (CGF). Pendant l'entra√Ænement, le conditionnement du texte est parfois laiss√© en blanc, ce qui oblige le mod√®le √† apprendre √† d√©bruiter les images sans aucune information textuelle (g√©n√©ration inconditionnelle). Ensuite, au moment de l'inf√©rence, nous faisons deux pr√©dictions distinctes : l'une avec le texte prompt comme conditionnement et l'autre sans. Nous pouvons ensuite utiliser la diff√©rence entre ces deux pr√©dictions pour cr√©er une pr√©diction combin√©e finale qui pousse **encore plus loin** dans la direction indiqu√©e par la pr√©diction conditionn√©e par le texte selon un certain facteur d'√©chelle (l'√©chelle de guidage), avec l'espoir d'obtenir une image qui corresponde mieux au prompt. L'image ci-dessus montre les r√©sultats d'un prompt √† diff√©rentes √©chelles de guidage. Comme vous pouvez le voir, des valeurs plus √©lev√©es donnent des images qui correspondent mieux √† la description.

## Autres types de conditionnement : super-r√©solution, peinture et profondeur d'image

Il est possible de cr√©er des versions de Stable Diffusion qui prennent en compte d'autres types de conditionnement. Par exemple, le mod√®le [Depth-to-Image model](https://huggingface.co/stabilityai/stable-diffusion-2-depth) poss√®de des canaux d'entr√©e suppl√©mentaires qui recueillent des informations approfondies sur l'image en cours de d√©bruitage et, au moment de l'inf√©rence, nous pouvons introduire la carte de profondeur d'une image cible (estim√©e √† l'aide d'un mod√®le distinct) pour esp√©rer g√©n√©rer une image dont la structure globale est similaire. 

![depth to image example](https://huggingface.co/stabilityai/stable-diffusion-2-depth/resolve/main/depth2image.png)<br>
_SD conditionn√© par la profondeur est capable de g√©n√©rer des images diff√©rentes avec la m√™me structure globale (exemple provenant de StabilityAI)_

De la m√™me mani√®re, nous pouvons introduire une image basse r√©solution comme conditionnement et demander au mod√®le de g√©n√©rer la version haute r√©solution ([comme utilis√© par le Stable Diffusion Upscaler](https://huggingface.co/stabilityai/stable-diffusion-x4-upscaler)). Enfin, nous pouvons introduire un masque montrant une r√©gion de l'image √† r√©g√©n√©rer dans le cadre de la t√¢che de compl√©tion d'image (*inpainting*), o√π les r√©gions non masqu√©es doivent rester intactes tandis que le nouveau contenu est g√©n√©r√© pour la zone masqu√©e. 

## Finetuning avec DreamBooth

![dreambooth diagram](https://dreambooth.github.io/DreamBooth_files/teaser_static.jpg)
_Image provenant de la [page du projet dreambooth] (https://dreambooth.github.io/) bas√©e sur le mod√®le Imagen_

DreamBooth est une technique permettant de *finetuner* un mod√®le texte-image afin de lui "apprendre" un nouveau concept, tel qu'un objet ou un style sp√©cifique. La technique a √©t√© d√©velopp√©e √† l'origine pour le mod√®le Imagen de Google, mais a √©t√© rapidement adapt√©e pour [fonctionner pour Stable Diffusion](https://huggingface.co/docs/diffusers/training/dreambooth). Les r√©sultats peuvent √™tre extr√™mement impressionnants (si vous avez vu quelqu'un avec une photo de profil IA sur les m√©dias sociaux r√©cemment, il y a de fortes chances qu'elle provienne d'un service bas√© sur Dreambooth), mais la technique est aussi sensible aux param√®tres utilis√©s, alors consultez notre *notebook* et [cet article de blog sur les diff√©rents param√®tres d'entra√Ænement](https://huggingface.co/blog/dreambooth) pour obtenir des conseils sur la fa√ßon de la faire fonctionner le mieux possible.

## Notebooks

| Chapitre                                    | Colab                                                                                                                                                                                               | Kaggle                                                                                                                                                                                                   | Gradient                                                                                                                                                                               | Studio Lab                                                                                                                                                                                                   |
|:--------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Introduction √† Stable Diffusion             | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/diffusion-models-class/blob/main/fr/unit3/_stable_diffusion_introduction.ipynb)              | [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/huggingface/diffusion-models-class/blob/main/fr/unit3/_stable_diffusion_introduction.ipynb)              | [![Gradient](https://assets.paperspace.io/img/gradient-badge.svg)](https://console.paperspace.com/github/huggingface/diffusion-models-class/blob/main/fr/unit3/_stable_diffusion_introduction.ipynb)              | [![Open In SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/diffusion-models-class/blob/main/fr/unit3/_stable_diffusion_introduction.ipynb)              |
| Plong√©e dans Stable Diffusion               | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/fastai/diffusion-nbs/blob/master/Stable%20Diffusion%20Deep%20Dive.ipynb )              | [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/fastai/diffusion-nbs/blob/master/Stable%20Diffusion%20Deep%20Dive.ipynb )              | [![Gradient](https://assets.paperspace.io/img/gradient-badge.svg)](https://console.paperspace.com/github/fastai/diffusion-nbs/blob/master/Stable%20Diffusion%20Deep%20Dive.ipynb )              | [![Open In SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/fastai/diffusion-nbs/blob/master/Stable%20Diffusion%20Deep%20Dive.ipynb )              |

Le *notebook Stable Diffusion Introduction* est une courte introduction √† Stable Diffusion avec la biblioth√®que ü§ó *Diffusers*, pr√©sentant quelques exemples d'utilisation de base en utilisant des pipelines pour g√©n√©rer et modifier des images.

Enfin, le *notebook* et la vid√©o *Stable Diffusion Deep Dive* d√©composent chaque √©tape d'un pipeline de g√©n√©ration typique, en sugg√©rant de nouvelles fa√ßons de modifier chaque √©tape pour un contr√¥le cr√©atif suppl√©mentaire. 


## Project

LIEN VERS L'EVENT

## Ressources compl√©mentaires
Une liste non exhaustive de ressources (en anglais) √† consulter :
- [High-Resolution Image Synthesis with Latent Diffusion Models](http://arxiv.org/abs/2112.10752), le papier qui a introduit l'approche derri√®re Stable Diffusion
- [CLIP](https://openai.com/blog/clip/) apprend √† relier le texte aux images et l'encodeur textuel est utilis√© pour transformer un prompt textuel en la riche repr√©sentation num√©rique utilis√©e par SD. Voir √©galement [cet article sur OpenCLIP] (https://wandb.ai/johnowhitaker/openclip-benchmarking/reports/Exploring-OpenCLIP--VmlldzoyOTIzNzIz) pour en savoir plus sur les r√©centes variantes de CLIP open-source (dont l'une est utilis√©e pour la version 2 de SD).
- [GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models](https://arxiv.org/abs/2112.10741) un papier pr√©coce d√©montrant le conditionnement de texte et le CFG

Vous avez identifi√© d'autres ressources int√©ressantes ? Faites-le nous savoir et nous les ajouterons √† cette liste.