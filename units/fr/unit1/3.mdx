# Impl√©mentation √† partir de 0

<CourseFloatingBanner unit={1}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
	{label: "Impl√©mentation √† partir de 0", value: "https://colab.research.google.com/github/huggingface/diffusion-models-class/blob/main/units/fr/unit1/diffusion_models_from_scratch.ipynb"},
    {label: "Impl√©mentation √† partir de 0", value: "https://studiolab.sagemaker.aws/import/github/huggingface/diffusion-models-class/blob/main/units/fr/unit1/diffusion_models_from_scratch.ipynb"},

]} />


Il est parfois utile de consid√©rer la version la plus simple possible d'une chose pour mieux en comprendre le fonctionnement. C'est ce que nous allons essayer de faire dans ce *notebook*, en commen√ßant par un mod√®le de diffusion jouet pour voir comment les diff√©rents √©l√©ments fonctionnent, puis en examinant en quoi ils diff√®rent d'une mise en ≈ìuvre plus complexe.

Nous examinerons :
- Le processus de corruption (ajouter du bruit aux donn√©es)
- Ce qu'est un UNet, et comment en impl√©menter un extr√™mement minimal √† partir de z√©ro
- L'entra√Ænement au mod√®le de diffusion
- La th√©orie de l'√©chantillonnage

Ensuite, nous comparerons nos versions avec l'impl√©mentation DDPM des diffuseurs, en explorant :
- Les am√©liorations par rapport √† notre mini UNet
- Le sch√©ma de bruit du DDPM
- Les diff√©rences dans l'objectif d'entra√Ænement
- Le conditionnement du pas de temps
- Les approches d'√©chantillonnage

Ce *notebook* est assez approfondi, et peut √™tre saut√© en toute s√©curit√© si vous n'√™tes pas enthousiaste √† l'id√©e d'une plong√©e en profondeur √† partir de z√©ro !

Il convient √©galement de noter que la plupart du code ici est utilis√© √† des fins d'illustration, et nous ne recommandons pas de l'adopter directement pour votre propre travail (√† moins que vous n'essayiez d'am√©liorer les exemples montr√©s ici √† des fins d'apprentissage).

## Configuration et importations

```py
!pip install -q diffusers
```

```py
import torch
import torchvision
from torch import nn
from torch.nn import functional as F
from torch.utils.data import DataLoader
from diffusers import DDPMScheduler, UNet2DModel
from matplotlib import pyplot as plt

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f'Using device: {device}')
```

## Les donn√©es

Nous allons tester les choses avec un tr√®s petit jeu de donn√©es : MNIST. Si vous souhaitez donner au mod√®le un d√©fi un peu plus difficile √† relever sans rien changer d'autre, `torchvision.datasets.FashionMNIST` devrait faire l'affaire.

```py
dataset = torchvision.datasets.MNIST(root="mnist/", train=True, download=True, transform=torchvision.transforms.ToTensor())
```

```py
train_dataloader = DataLoader(dataset, batch_size=8, shuffle=True)
```

```py
x, y = next(iter(train_dataloader))
print('Input shape:', x.shape)
print('Labels:', y)
plt.imshow(torchvision.utils.make_grid(x)[0], cmap='Greys')
```
```py
Input shape: torch.Size([8, 1, 28, 28]) 
Labels: tensor([1, 9, 7, 3, 5, 2, 1, 4])
```

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary.svg" alt="Bref aper√ßu du contenu du cours.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary-dark.svg" alt="Bref aper√ßu des diff√©rents chapitres du cours.">
</div>

Chaque image est un dessin en niveaux de gris de 28 par 28 pixels d'un chiffre, avec des valeurs allant de 0 √† 1.


## Le processus de corruption

Supposons que vous n'ayez lu aucun papier sur les mod√®les de diffusion, mais que vous sachiez que le processus implique l'ajout de bruit. Comment feriez-vous ?

Nous souhaitons probablement disposer d'un moyen simple de contr√¥ler le degr√© de corruption. Et si nous prenions un param√®tre pour la quantit√© de bruit √† ajouter, et que nous le faisions :

```py
noise = torch.rand_like(x)
```

```py
noisy_x = (1-amount)*x + amount*noise
```

Si `amount = 0`, nous r√©cup√©rons l'entr√©e sans aucun changement. Si le montant atteint $1$, nous r√©cup√©rons du bruit sans aucune trace de l'entr√©e $x$. En m√©langeant l'entr√©e avec du bruit de cette fa√ßon, nous gardons la sortie dans la m√™me plage ($0$ √† $1$).

Nous pouvons mettre cela en ≈ìuvre assez facilement (il suffit de surveiller les formes pour ne pas se faire pi√©ger par les r√®gles de diffusion) :

```py
def corrupt(x, amount):
  """Corrompre l'entr√©e `x` en la m√©langeant avec du bruit selon `amount`"""
  noise = torch.rand_like(x)
  amount = amount.view(-1, 1, 1, 1) # Trier les formes pour que la transmission fonctionne
  return x*(1-amount) + noise*amount 
```

Et regarder les r√©sultats visuellement pour voir que cela fonctionne comme pr√©vu :

```py
# Tracer les donn√©es d'entr√©e
fig, axs = plt.subplots(2, 1, figsize=(12, 5))
axs[0].set_title('Input data')
axs[0].imshow(torchvision.utils.make_grid(x)[0], cmap='Greys')

# Ajouter du bruit
amount = torch.linspace(0, 1, x.shape[0]) # De gauche √† droite -> plus de corruption
noised_x = corrupt(x, amount)

# Trac√© de la version bruit√©e
axs[1].set_title('Corrupted data (-- amount increases -->)')
axs[1].imshow(torchvision.utils.make_grid(noised_x)[0], cmap='Greys')
```

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary.svg" alt="Bref aper√ßu du contenu du cours.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary-dark.svg" alt="Bref aper√ßu des diff√©rents chapitres du cours.">
</div>

Lorsque la quantit√© de bruit s'approche de 1, nos donn√©es commencent √† ressembler √† du bruit al√©atoire pur. Mais pour la plupart des `noise_amounts`, vous pouvez deviner le chiffre assez bien. Pensez-vous que cela soit optimal ?

## Le mod√®le

Nous aimerions un mod√®le qui prenne en compte des images bruit√©es de 28px et qui produise une pr√©diction de la m√™me forme. Un choix populaire ici est une architecture appel√©e UNet. Invent√© √† l'origine pour les t√¢ches de [segmentation en imagerie m√©dicale](https://arxiv.org/abs/1505.04597), un UNet se compose d'un "chemin de compression" par lequel les donn√©es sont comprim√©es et d'un "chemin d'expansion" par lequel elles s'√©tendent √† nouveau jusqu'√† la dimension d'origine (similaire √† un autoencodeur), mais il comporte √©galement des connexions de saut qui permettent aux informations et aux gradients de circuler √† diff√©rents niveaux.

Certains UNets comportent des blocs complexes √† chaque √©tape, mais pour cette petite d√©monstration, nous construirons un exemple minimal qui prend une image √† un canal et la fait passer par trois couches convolutives sur le chemin descendant (les down_layers dans le diagramme et le code) et trois sur le chemin ascendant, avec des sauts de connexion entre les couches descendantes et ascendantes. Nous utiliserons max pooling pour le downsampling et `nn.Upsample` pour le upsampling plut√¥t que de nous appuyer sur des couches apprenantes comme les UNets plus complexes. Voici l'architecture approximative montrant le nombre de canaux dans la sortie de chaque couche :

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary.svg" alt="Bref aper√ßu du contenu du cours.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary-dark.svg" alt="Bref aper√ßu des diff√©rents chapitres du cours.">
</div>

Voici √† quoi cela ressemble dans le code :

```py
class BasicUNet(nn.Module):
    """Une mise en ≈ìuvre minimale du UNet"""
    def __init__(self, in_channels=1, out_channels=1):
        super().__init__()
        self.down_layers = torch.nn.ModuleList([ 
            nn.Conv2d(in_channels, 32, kernel_size=5, padding=2),
            nn.Conv2d(32, 64, kernel_size=5, padding=2),
            nn.Conv2d(64, 64, kernel_size=5, padding=2),
        ])
        self.up_layers = torch.nn.ModuleList([
            nn.Conv2d(64, 64, kernel_size=5, padding=2),
            nn.Conv2d(64, 32, kernel_size=5, padding=2),
            nn.Conv2d(32, out_channels, kernel_size=5, padding=2), 
        ])
        self.act = nn.SiLU() # La fonction d'activation
        self.downscale = nn.MaxPool2d(2)
        self.upscale = nn.Upsample(scale_factor=2)

    def forward(self, x):
        h = []
        for i, l in enumerate(self.down_layers):
            x = self.act(l(x)) # √Ä travers la couche et la fonction d'activation
            if i < 2: # Pour toutes les couches sauf la troisi√®me (derni√®re) :
              h.append(x) # Stockage de la sortie pour la skip connexion 
              x = self.downscale(x) # R√©duction d'√©chelle pour la couche suivante
              
        for i, l in enumerate(self.up_layers):
            if i > 0:
              x = self.upscale(x) # Upscale
              x += h.pop() # R√©cup√©ration d'un r√©sultat stock√© (skip connection)
            x = self.act(l(x)) # Par le biais de la couche et de la fonction d'activation
            
        return x
```

Nous pouvons v√©rifier que la forme de la sortie est la m√™me que celle de l'entr√©e, comme nous nous y attendions :

```py
net = BasicUNet()
x = torch.rand(8, 1, 28, 28)
net(x).shape
```
```py
torch.Size([8, 1, 28, 28])
```
Ce r√©seau compte un peu plus de 300 000 param√®tres :

```py
sum([p.numel() for p in net.parameters()])
```

```py
309057
```

Vous pouvez envisager de modifier le nombre de canaux dans chaque couche ou d'intervertir les architectures si vous le souhaitez.

# Entra√Æner le r√©seau

Que doit faire exactement le mod√®le ? L√† encore, il y a plusieurs fa√ßons de proc√©der, mais pour cette d√©monstration, choisissons un cadre simple : √©tant donn√© une entr√©e corrompue `noisy_x`, le mod√®le doit produire sa meilleure estimation de ce √† quoi ressemble l'original $x$. Nous comparerons cette valeur √† la valeur r√©elle par le biais de l'erreur quadratique moyenne. Nous comparerons cette estimation √† la valeur r√©elle par le biais de l'erreur quadratique moyenne.

Nous pouvons maintenant entra√Æner le r√©seau.
- Obtenir un batch de donn√©es
- Corrompre les donn√©es de mani√®re al√©atoire
- Nourrir le mod√®le avec ces donn√©es
- Comparer les pr√©dictions du mod√®le avec les images propres pour calculer notre perte
- Mettre √† jour les param√®tres du mod√®le en cons√©quence.

N'h√©sitez pas √† modifier ce mod√®le et √† voir si vous pouvez l'am√©liorer !

```py
# Chargeur de donn√©es (vous pouvez modifier la taille des batchs)
batch_size = 128
train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

# Combien de fois devrions-nous passer les donn√©es en revue ?
n_epochs = 3

# Cr√©er le r√©seau
net = BasicUNet()
net.to(device)

# Notre fonction de perte
loss_fn = nn.MSELoss()

# L'optimiseur
opt = torch.optim.Adam(net.parameters(), lr=1e-3) 

# Conserver une trace des pertes pour les consulter ult√©rieurement
losses = []

# La boucle d'entra√Ænement
for epoch in range(n_epochs):

    for x, y in train_dataloader:

        # Obtenir des donn√©es et pr√©parer la version corrompue
        x = x.to(device) # Data on the GPU
        noise_amount = torch.rand(x.shape[0]).to(device) # Pick random noise amounts
        noisy_x = corrupt(x, noise_amount) # Create our noisy x

        # Obtenir la pr√©diction du mod√®le
        pred = net(noisy_x)

        # Calculer la perte
        loss = loss_fn(pred, x) # Dans quelle mesure la sortie est-elle proche du v√©ritable x "propre" ?

        # R√©tropropager et mettre √† jour les param√®tres
        opt.zero_grad()
        loss.backward()
        opt.step()

        # Stocker la perte pour plus tard
        losses.append(loss.item())

    # Afficher la moyenne des valeurs de perte pour cette √©poque :
    avg_loss = sum(losses[-len(train_dataloader):])/len(train_dataloader)
    print(f'Finished epoch {epoch}. Average loss for this epoch: {avg_loss:05f}')

# Visualiser la courbe des pertes
plt.plot(losses)
plt.ylim(0, 0.1)
```
```py
Finished epoch 0. Average loss for this epoch: 0.026736 
Finished epoch 1. Average loss for this epoch: 0.020692 
Finished epoch 2. Average loss for this epoch: 0.018887
```

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary.svg" alt="Bref aper√ßu du contenu du cours.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary-dark.svg" alt="Bref aper√ßu des diff√©rents chapitres du cours.">
</div>

Nous pouvons essayer de voir √† quoi ressemblent les pr√©dictions du mod√®le en saisissant un batch de donn√©es, en les corrompant √† diff√©rents degr√©s et en visualisant ensuite les pr√©dictions du mod√®le :

```py
# R√©cup√©rer des donn√©es
x, y = next(iter(train_dataloader))
x = x[:8] # Seuls les 8 premiers sont utilis√©s pour faciliter le graphique

# Corruption avec une √©chelle de montants
amount = torch.linspace(0, 1, x.shape[0]) # De gauche √† droite -> plus de corruption
noised_x = corrupt(x, amount)

# Obtenir les pr√©dictions du mod√®le
with torch.no_grad():
  preds = net(noised_x.to(device)).detach().cpu()

# Graphique
fig, axs = plt.subplots(3, 1, figsize=(12, 7))
axs[0].set_title('Input data')
axs[0].imshow(torchvision.utils.make_grid(x)[0].clip(0, 1), cmap='Greys')
axs[1].set_title('Corrupted data')
axs[1].imshow(torchvision.utils.make_grid(noised_x)[0].clip(0, 1), cmap='Greys')
axs[2].set_title('Network Predictions')
axs[2].imshow(torchvision.utils.make_grid(preds)[0].clip(0, 1), cmap='Greys
```

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary.svg" alt="Bref aper√ßu du contenu du cours.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary-dark.svg" alt="Bref aper√ßu des diff√©rents chapitres du cours.">
</div>

Vous pouvez constater que pour les montants les plus faibles, les pr√©dictions sont plut√¥t bonnes ! Mais lorsque le niveau devient tr√®s √©lev√©, le mod√®le a moins d'√©l√©ments pour travailler, et lorsque nous arrivons √† amount=1, il produit un d√©sordre flou proche de la moyenne du jeu de donn√©es pour essayer de couvrir ses paris sur ce √† quoi la sortie pourrait ressembler...

## √âchantillonnage

Si nos pr√©dictions √† des niveaux de bruit √©lev√©s ne sont pas tr√®s bonnes, comment g√©n√©rer des images ?

Et si nous partions d'un bruit al√©atoire, que nous regardions les pr√©dictions du mod√®le, mais que nous ne nous rapprochions que tr√®s peu de cette pr√©diction (disons, 20 % du chemin). Nous disposons alors d'une image tr√®s bruyante dans laquelle il y a peut-√™tre un soup√ßon de structure, que nous pouvons introduire dans le mod√®le pour obtenir une nouvelle pr√©diction. Nous esp√©rons que cette nouvelle pr√©diction est l√©g√®rement meilleure que la premi√®re (puisque notre point de d√©part est l√©g√®rement moins bruit√©) et que nous pouvons donc faire un autre petit pas avec cette nouvelle et meilleure pr√©diction.

Nous r√©p√©tons l'op√©ration plusieurs fois et (si tout se passe bien) nous obtenons une image ! Voici ce processus illustr√© en seulement 5 √©tapes, en visualisant l'entr√©e du mod√®le (√† gauche) et les images d√©bruit√©es pr√©dites (√† droite) √† chaque √©tape. Notez que m√™me si le mod√®le pr√©dit l'image d√©bruit√©e d√®s l'√©tape 1, nous ne faisons qu'une partie du chemin. Au fil des √©tapes, les structures apparaissent et sont affin√©es, jusqu'√† ce que nous obtenions nos r√©sultats finaux.

```py
n_steps = 5
x = torch.rand(8, 1, 28, 28).to(device) # Commencer au hasard
step_history = [x.detach().cpu()]
pred_output_history = []

for i in range(n_steps):
    with torch.no_grad(): # Pas besoin de suivre les gradients pendant l'inf√©rence
        pred = net(x) # Pr√©dire le x0 d√©bruit√©
    pred_output_history.append(pred.detach().cpu()) # Stocker les r√©sultats du mod√®le pour les tracer
    mix_factor = 1/(n_steps - i) # Dans quelle mesure nous nous rapprochons de la pr√©diction
    x = x*(1-mix_factor) + pred*mix_factor # D√©placer une partie du chemin
    step_history.append(x.detach().cpu()) # Stocker l'√©tape pour le graphique

fig, axs = plt.subplots(n_steps, 2, figsize=(9, 4), sharex=True)
axs[0,0].set_title('x (model input)')
axs[0,1].set_title('model prediction')
for i in range(n_steps):
    axs[i, 0].imshow(torchvision.utils.make_grid(step_history[i])[0].clip(0, 1), cmap='Greys')
    axs[i, 1].imshow(torchvision.utils.make_grid(pred_output_history[i])[0].clip(0, 1), cmap='Greys')
```

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary.svg" alt="Bref aper√ßu du contenu du cours.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary-dark.svg" alt="Bref aper√ßu des diff√©rents chapitres du cours.">
</div>

Nous pouvons diviser le processus en plusieurs √©tapes et esp√©rer ainsi obtenir de meilleures images :

```py
n_steps = 40
x = torch.rand(64, 1, 28, 28).to(device)
for i in range(n_steps):
  noise_amount = torch.ones((x.shape[0], )).to(device) * (1-(i/n_steps)) # Starting high going low
  with torch.no_grad():
    pred = net(x)
  mix_factor = 1/(n_steps - i)
  x = x*(1-mix_factor) + pred*mix_factor
fig, ax = plt.subplots(1, 1, figsize=(12, 12))
ax.imshow(torchvision.utils.make_grid(x.detach().cpu(), nrow=8)[0].clip(0, 1), cmap='Greys')
```

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary.svg" alt="Bref aper√ßu du contenu du cours.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary-dark.svg" alt="Bref aper√ßu des diff√©rents chapitres du cours.">
</div>

Ce n'est pas g√©nial, mais il y a des chiffres reconnaissables ! Vous pouvez exp√©rimenter en entra√Ænant plus longtemps (disons, 10 ou 20 √©poques) et en modifiant la configuration du mod√®le, le taux d'apprentissage, l'optimiseur, etc. N'oubliez pas non plus que fashionMNIST peut √™tre remplac√© en une ligne si vous voulez essayer un jeu de donn√©es un peu plus difficile.

## Comparaison avec DDPM

Dans cette section, nous allons voir comment notre impl√©mentation diff√®re de l'approche utilis√©e dans l'autre *notebook* (Introduction √† *Diffusers*), qui est bas√© sur l'article de DDPM.

Nous verrons que
- Le diffuseur `UNet2DModel` est un peu plus avanc√© que notre BasicUNet
- Le processus de corruption est trait√© diff√©remment
- L'objectif d'entra√Ænement est diff√©rent, puisqu'il s'agit de pr√©dire le bruit plut√¥t que l'image d√©bruit√©e.
- Le mod√®le est conditionn√© sur la quantit√© de bruit pr√©sent via un conditionnement par pas de temps, o√π t est transmis comme un argument suppl√©mentaire √† la m√©thode forward.
- Il existe un certain nombre de strat√©gies d'√©chantillonnage diff√©rentes, qui devraient fonctionner mieux que notre version simpliste ci-dessus.

Un certain nombre d'am√©liorations ont √©t√© sugg√©r√©es depuis la publication de l'article sur le DDPM, mais nous esp√©rons que cet exemple est instructif en ce qui concerne les diff√©rentes d√©cisions de conception possibles. Une fois que vous aurez lu cet article, vous pourrez vous plonger dans le document intitul√© [*Elucidating the Design Space of Diffusion-Based Generative Models*](https://arxiv.org/abs/2206.00364) qui examine tous ces composants en d√©tail et formule de nouvelles recommandations sur la mani√®re d'obtenir les meilleures performances.

Si tout cela est trop technique ou intimidant, ne vous inqui√©tez pas ! N'h√©sitez pas √† sauter le reste de ce *notebook* ou √† le garder pour un jour de pluie.

## L'UNet

Le mod√®le UNet2DModel de ü§ó *Diffusers* comporte un certain nombre d'am√©liorations par rapport √† notre UNet de base ci-dessus :
- GroupNorm applique une normalisation par groupe aux entr√©es de chaque bloc
- Couches de *dropout* pour un entra√Ænement plus doux
- Plusieurs couches de ResNet par bloc (si layers_per_block n'est pas fix√© √† 1)
- Attention (g√©n√©ralement utilis√© uniquement pour les blocs √† faible r√©solution)
- Conditionnement sur le pas de temps
- Blocs de sous-√©chantillonnage et de sur√©chantillonnage avec des param√®tres pouvant √™tre appris

Cr√©ons et inspectons un mod√®le UNet2DModel :

```py
model = UNet2DModel(
    sample_size=28,           # la r√©solution de l'image cible
    in_channels=1,            # le nombre de canaux d'entr√©e, 3 pour les images RVB
    out_channels=1,           # le nombre de canaux de sortie
    layers_per_block=2,       # le nombre de couches ResNet √† utiliser par bloc UNet
    block_out_channels=(32, 64, 64), # Correspondant √† peu pr√®s √† notre exemple UNet de base
    down_block_types=( 
        "DownBlock2D",        # un bloc de sous-√©chantillonnage ResNet normal
        "AttnDownBlock2D",    # un bloc de sous-√©chantillonnage ResNet avec auto-attention spatiale
        "AttnDownBlock2D",
    ), 
    up_block_types=(
        "AttnUpBlock2D", 
        "AttnUpBlock2D",      # un bloc de sur√©chantillonnage ResNet avec auto-attention spatiale
        "UpBlock2D",          # un bloc de sur√©chantillonnage ResNet standard
      ),
)
print(model)
```

```py
UNet2DModel(
  (conv_in): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (time_proj): Timesteps()
  (time_embedding): TimestepEmbedding(
    (linear_1): Linear(in_features=32, out_features=128, bias=True)
    (act): SiLU()
    (linear_2): Linear(in_features=128, out_features=128, bias=True)
  )
  (down_blocks): ModuleList(
    (0): DownBlock2D(
      (resnets): ModuleList(
        (0): ResnetBlock2D(
          (norm1): GroupNorm(32, 32, eps=1e-05, affine=True)
          (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (time_emb_proj): Linear(in_features=128, out_features=32, bias=True)
          (norm2): GroupNorm(32, 32, eps=1e-05, affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (nonlinearity): SiLU()
        )
        (1): ResnetBlock2D(
          (norm1): GroupNorm(32, 32, eps=1e-05, affine=True)
          (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (time_emb_proj): Linear(in_features=128, out_features=32, bias=True)
          (norm2): GroupNorm(32, 32, eps=1e-05, affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (nonlinearity): SiLU()
        )
      )
      (downsamplers): ModuleList(
        (0): Downsample2D(
          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        )
      )
    )
    (1): AttnDownBlock2D(
      (attentions): ModuleList(
        (0): AttentionBlock(
          (group_norm): GroupNorm(32, 64, eps=1e-05, affine=True)
          (query): Linear(in_features=64, out_features=64, bias=True)
          (key): Linear(in_features=64, out_features=64, bias=True)
          (value): Linear(in_features=64, out_features=64, bias=True)
          (proj_attn): Linear(in_features=64, out_features=64, bias=True)
        )
        (1): AttentionBlock(
          (group_norm): GroupNorm(32, 64, eps=1e-05, affine=True)
          (query): Linear(in_features=64, out_features=64, bias=True)
          (key): Linear(in_features=64, out_features=64, bias=True)
          (value): Linear(in_features=64, out_features=64, bias=True)
          (proj_attn): Linear(in_features=64, out_features=64, bias=True)
        )
      )
      (resnets): ModuleList(
        (0): ResnetBlock2D(
          (norm1): GroupNorm(32, 32, eps=1e-05, affine=True)
          (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (time_emb_proj): Linear(in_features=128, out_features=64, bias=True)
          (norm2): GroupNorm(32, 64, eps=1e-05, affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (nonlinearity): SiLU()
          (conv_shortcut): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))
        )
        (1): ResnetBlock2D(
          (norm1): GroupNorm(32, 64, eps=1e-05, affine=True)
          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (time_emb_proj): Linear(in_features=128, out_features=64, bias=True)
          (norm2): GroupNorm(32, 64, eps=1e-05, affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (nonlinearity): SiLU()
        )
      )
      (downsamplers): ModuleList(
        (0): Downsample2D(
          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        )
      )
    )
    (2): AttnDownBlock2D(
      (attentions): ModuleList(
        (0): AttentionBlock(
          (group_norm): GroupNorm(32, 64, eps=1e-05, affine=True)
          (query): Linear(in_features=64, out_features=64, bias=True)
          (key): Linear(in_features=64, out_features=64, bias=True)
          (value): Linear(in_features=64, out_features=64, bias=True)
          (proj_attn): Linear(in_features=64, out_features=64, bias=True)
        )
        (1): AttentionBlock(
          (group_norm): GroupNorm(32, 64, eps=1e-05, affine=True)
          (query): Linear(in_features=64, out_features=64, bias=True)
          (key): Linear(in_features=64, out_features=64, bias=True)
          (value): Linear(in_features=64, out_features=64, bias=True)
          (proj_attn): Linear(in_features=64, out_features=64, bias=True)
        )
      )
      (resnets): ModuleList(
        (0): ResnetBlock2D(
          (norm1): GroupNorm(32, 64, eps=1e-05, affine=True)
          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (time_emb_proj): Linear(in_features=128, out_features=64, bias=True)
          (norm2): GroupNorm(32, 64, eps=1e-05, affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (nonlinearity): SiLU()
        )
        (1): ResnetBlock2D(
          (norm1): GroupNorm(32, 64, eps=1e-05, affine=True)
          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (time_emb_proj): Linear(in_features=128, out_features=64, bias=True)
          (norm2): GroupNorm(32, 64, eps=1e-05, affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (nonlinearity): SiLU()
        )
      )
    )
  )
  (up_blocks): ModuleList(
    (0): AttnUpBlock2D(
      (attentions): ModuleList(
        (0): AttentionBlock(
          (group_norm): GroupNorm(32, 64, eps=1e-05, affine=True)
          (query): Linear(in_features=64, out_features=64, bias=True)
          (key): Linear(in_features=64, out_features=64, bias=True)
          (value): Linear(in_features=64, out_features=64, bias=True)
          (proj_attn): Linear(in_features=64, out_features=64, bias=True)
        )
        (1): AttentionBlock(
          (group_norm): GroupNorm(32, 64, eps=1e-05, affine=True)
          (query): Linear(in_features=64, out_features=64, bias=True)
          (key): Linear(in_features=64, out_features=64, bias=True)
          (value): Linear(in_features=64, out_features=64, bias=True)
          (proj_attn): Linear(in_features=64, out_features=64, bias=True)
        )
        (2): AttentionBlock(
          (group_norm): GroupNorm(32, 64, eps=1e-05, affine=True)
          (query): Linear(in_features=64, out_features=64, bias=True)
          (key): Linear(in_features=64, out_features=64, bias=True)
          (value): Linear(in_features=64, out_features=64, bias=True)
          (proj_attn): Linear(in_features=64, out_features=64, bias=True)
        )
      )
      (resnets): ModuleList(
        (0): ResnetBlock2D(
          (norm1): GroupNorm(32, 128, eps=1e-05, affine=True)
          (conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (time_emb_proj): Linear(in_features=128, out_features=64, bias=True)
          (norm2): GroupNorm(32, 64, eps=1e-05, affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (nonlinearity): SiLU()
          (conv_shortcut): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
        )
        (1): ResnetBlock2D(
          (norm1): GroupNorm(32, 128, eps=1e-05, affine=True)
          (conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (time_emb_proj): Linear(in_features=128, out_features=64, bias=True)
          (norm2): GroupNorm(32, 64, eps=1e-05, affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (nonlinearity): SiLU()
          (conv_shortcut): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
        )
        (2): ResnetBlock2D(
          (norm1): GroupNorm(32, 128, eps=1e-05, affine=True)
          (conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (time_emb_proj): Linear(in_features=128, out_features=64, bias=True)
          (norm2): GroupNorm(32, 64, eps=1e-05, affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (nonlinearity): SiLU()
          (conv_shortcut): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
        )
      )
      (upsamplers): ModuleList(
        (0): Upsample2D(
          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
      )
    )
    (1): AttnUpBlock2D(
      (attentions): ModuleList(
        (0): AttentionBlock(
          (group_norm): GroupNorm(32, 64, eps=1e-05, affine=True)
          (query): Linear(in_features=64, out_features=64, bias=True)
          (key): Linear(in_features=64, out_features=64, bias=True)
          (value): Linear(in_features=64, out_features=64, bias=True)
          (proj_attn): Linear(in_features=64, out_features=64, bias=True)
        )
        (1): AttentionBlock(
          (group_norm): GroupNorm(32, 64, eps=1e-05, affine=True)
          (query): Linear(in_features=64, out_features=64, bias=True)
          (key): Linear(in_features=64, out_features=64, bias=True)
          (value): Linear(in_features=64, out_features=64, bias=True)
          (proj_attn): Linear(in_features=64, out_features=64, bias=True)
        )
        (2): AttentionBlock(
          (group_norm): GroupNorm(32, 64, eps=1e-05, affine=True)
          (query): Linear(in_features=64, out_features=64, bias=True)
          (key): Linear(in_features=64, out_features=64, bias=True)
          (value): Linear(in_features=64, out_features=64, bias=True)
          (proj_attn): Linear(in_features=64, out_features=64, bias=True)
        )
      )
      (resnets): ModuleList(
        (0): ResnetBlock2D(
          (norm1): GroupNorm(32, 128, eps=1e-05, affine=True)
          (conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (time_emb_proj): Linear(in_features=128, out_features=64, bias=True)
          (norm2): GroupNorm(32, 64, eps=1e-05, affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (nonlinearity): SiLU()
          (conv_shortcut): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
        )
        (1): ResnetBlock2D(
          (norm1): GroupNorm(32, 128, eps=1e-05, affine=True)
          (conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (time_emb_proj): Linear(in_features=128, out_features=64, bias=True)
          (norm2): GroupNorm(32, 64, eps=1e-05, affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (nonlinearity): SiLU()
          (conv_shortcut): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
        )
        (2): ResnetBlock2D(
          (norm1): GroupNorm(32, 96, eps=1e-05, affine=True)
          (conv1): Conv2d(96, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (time_emb_proj): Linear(in_features=128, out_features=64, bias=True)
          (norm2): GroupNorm(32, 64, eps=1e-05, affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (nonlinearity): SiLU()
          (conv_shortcut): Conv2d(96, 64, kernel_size=(1, 1), stride=(1, 1))
        )
      )
      (upsamplers): ModuleList(
        (0): Upsample2D(
          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
      )
    )
    (2): UpBlock2D(
      (resnets): ModuleList(
        (0): ResnetBlock2D(
          (norm1): GroupNorm(32, 96, eps=1e-05, affine=True)
          (conv1): Conv2d(96, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (time_emb_proj): Linear(in_features=128, out_features=32, bias=True)
          (norm2): GroupNorm(32, 32, eps=1e-05, affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (nonlinearity): SiLU()
          (conv_shortcut): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1))
        )
        (1): ResnetBlock2D(
          (norm1): GroupNorm(32, 64, eps=1e-05, affine=True)
          (conv1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (time_emb_proj): Linear(in_features=128, out_features=32, bias=True)
          (norm2): GroupNorm(32, 32, eps=1e-05, affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (nonlinearity): SiLU()
          (conv_shortcut): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        )
        (2): ResnetBlock2D(
          (norm1): GroupNorm(32, 64, eps=1e-05, affine=True)
          (conv1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (time_emb_proj): Linear(in_features=128, out_features=32, bias=True)
          (norm2): GroupNorm(32, 32, eps=1e-05, affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (nonlinearity): SiLU()
          (conv_shortcut): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        )
      )
    )
  )
  (mid_block): UNetMidBlock2D(
    (attentions): ModuleList(
      (0): AttentionBlock(
        (group_norm): GroupNorm(32, 64, eps=1e-05, affine=True)
        (query): Linear(in_features=64, out_features=64, bias=True)
        (key): Linear(in_features=64, out_features=64, bias=True)
        (value): Linear(in_features=64, out_features=64, bias=True)
        (proj_attn): Linear(in_features=64, out_features=64, bias=True)
      )
    )
    (resnets): ModuleList(
      (0): ResnetBlock2D(
        (norm1): GroupNorm(32, 64, eps=1e-05, affine=True)
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (time_emb_proj): Linear(in_features=128, out_features=64, bias=True)
        (norm2): GroupNorm(32, 64, eps=1e-05, affine=True)
        (dropout): Dropout(p=0.0, inplace=False)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (nonlinearity): SiLU()
      )
      (1): ResnetBlock2D(
        (norm1): GroupNorm(32, 64, eps=1e-05, affine=True)
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (time_emb_proj): Linear(in_features=128, out_features=64, bias=True)
        (norm2): GroupNorm(32, 64, eps=1e-05, affine=True)
        (dropout): Dropout(p=0.0, inplace=False)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (nonlinearity): SiLU()
      )
    )
  )
  (conv_norm_out): GroupNorm(32, 32, eps=1e-05, affine=True)
  (conv_act): SiLU()
  (conv_out): Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
)
```

Comme vous pouvez le constater, il y a un peu plus de choses qui se passent ! Il a √©galement beaucoup plus de param√®tres que notre BasicUNet :

```py
sum([p.numel() for p in model.parameters()]) # 1,7M contre les ~309k param√®tres du BasicUNet
```

```py
1707009
```

Nous pouvons reproduire l'entra√Ænement pr√©sent√© ci-dessus en utilisant ce mod√®le √† la place de notre mod√®le original. Nous devons passer x et le pas de temps au mod√®le (ici, nous passons toujours t=0 pour montrer qu'il fonctionne sans ce conditionnement de pas de temps et pour faciliter le code d'√©chantillonnage, mais vous pouvez √©galement essayer d'introduire `(amount*1000)` pour obtenir un √©quivalent de pas de temps √† partir du montant de la corruption). Les lignes modifi√©es sont indiqu√©es par `#<<<` si vous souhaitez inspecter le code.

```py
# Dataloader (vous pouvez modifier la taille du batch)
batch_size = 128
train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

# Combien de fois devrions-nous passer les donn√©es en revue ?
n_epochs = 3

# Cr√©er le r√©seau
net = UNet2DModel(
    sample_size=28,           # la r√©solution de l'image cible
    in_channels=1,            # le nombre de canaux d'entr√©e, 3 pour les images RVB
    out_channels=1,           # le nombre de canaux de sortie
    layers_per_block=2,       # le nombre de couches ResNet √† utiliser par bloc UNet
    block_out_channels=(32, 64, 64), # Correspondant √† peu pr√®s √† notre exemple UNet de base
    down_block_types=( 
        "DownBlock2D",        # un bloc de sous-√©chantillonnage ResNet normal
        "AttnDownBlock2D",    # un bloc de sous-√©chantillonnage ResNet avec auto-attention spatiale
        "AttnDownBlock2D",
    ), 
    up_block_types=(
        "AttnUpBlock2D", 
        "AttnUpBlock2D",      # un bloc de sur√©chantillonnage ResNet avec auto-attention spatiale
        "UpBlock2D",          # un bloc de sur√©chantillonnage ResNet standard
      ),
)

net.to(device)

# Notre protection contre la perte
loss_fn = nn.MSELoss()

# L'optimiseur
opt = torch.optim.Adam(net.parameters(), lr=1e-3) 

# Conserver une trace des pertes pour les visualiser plus tard
losses = []

# La boucle d'entra√Ænement
for epoch in range(n_epochs):

    for x, y in train_dataloader:

        # Obtenir des donn√©es et pr√©parer la version corrompue
        x = x.to(device) # Data on the GPU
        noise_amount = torch.rand(x.shape[0]).to(device) # Choisir des quantit√©s de bruit al√©atoires
        noisy_x = corrupt(x, noise_amount) # Cr√©er notre bruit x

        # Obtenir la pr√©diction du mod√®le
        pred = net(noisy_x, 0).sample #<<< En utilisant toujours le pas de temps 0, en ajoutant .sample

        # Calculer la perte
        loss = loss_fn(pred, x) # Dans quelle mesure la sortie est-elle proche du v√©ritable x "propre" ?

        # R√©tropropager et mettre √† jour les param√®tres
        opt.zero_grad()
        loss.backward()
        opt.step()

        # Stocker la perte pour plus tard
        losses.append(loss.item())

    # Afficher la moyenne des valeurs de perte pour cette √©poque :
    avg_loss = sum(losses[-len(train_dataloader):])/len(train_dataloader)
    print(f'Finished epoch {epoch}. Average loss for this epoch: {avg_loss:05f}')

# Graphique
fig, axs = plt.subplots(1, 2, figsize=(12, 5))

# Perte
axs[0].plot(losses)
axs[0].set_ylim(0, 0.1)
axs[0].set_title('Loss over time')

# √âchantillons
n_steps = 40
x = torch.rand(64, 1, 28, 28).to(device)
for i in range(n_steps):
  noise_amount = torch.ones((x.shape[0], )).to(device) * (1-(i/n_steps)) # De haut en bas
  with torch.no_grad():
    pred = net(x, 0).sample
  mix_factor = 1/(n_steps - i)
  x = x*(1-mix_factor) + pred*mix_factor

axs[1].imshow(torchvision.utils.make_grid(x.detach().cpu(), nrow=8)[0].clip(0, 1), cmap='Greys')
axs[1].set_title('Generated Samples')
```

```py
Finished epoch 0. Average loss for this epoch: 0.018925 
Finished epoch 1. Average loss for this epoch: 0.012785 
Finished epoch 2. Average loss for this epoch: 0.011694
```

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary.svg" alt="Bref aper√ßu du contenu du cours.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary-dark.svg" alt="Bref aper√ßu des diff√©rents chapitres du cours.">
</div>

Ces r√©sultats sont bien meilleurs que notre premi√®re s√©rie de r√©sultats ! Vous pouvez envisager de modifier la configuration du Unet ou de prolonger l'entra√Ænement afin d'obtenir des performances encore meilleures.

## Le processus de corruption

Le papier DDPM d√©crit un processus de corruption qui ajoute une petite quantit√© de bruit √† chaque "pas de temps". 
Etant donn√© $x_{t-1}$ pour un certain pas de temps, nous pouvons obtenir la version suivante (l√©g√®rement plus bruit√©e) $x_t$ avec :<br><br>

$q(\mathbf{x}_t \vert \mathbf{x}_{t-1}) = \mathcal{N}(\mathbf{x}_t; \sqrt{1 - \beta_t} \mathbf{x}_{t-1}, \beta_t\mathbf{I}) \quad
q(\mathbf{x}_{1:T} \vert \mathbf{x}_0) = \prod^T_{t=1} q(\mathbf{x}_t \vert \mathbf{x}_{t-1})$<br><br>


Nous prenons $x_{t-1}$, l'√©chelonnons de $\sqrt{1 - \beta_t}$ et ajoutons du bruit √©chelonn√© de $\beta_t$. 
Ce $\beta$ est d√©fini pour chaque t en fonction d'un certain planificateur, et d√©termine la quantit√© de bruit ajout√©e par pas de temps.
Nous ne voulons pas n√©cessairement faire cette op√©ration 500 fois pour obtenir $x_{500}$, nous avons donc une autre formule pour obtenir $x_t$ pour n'importe quel t √©tant donn√© $x_0$ : <br><br>

```
noise_scheduler = DDPMScheduler(num_train_timesteps=1000)
plt.plot(noise_scheduler.alphas_cumprod.cpu() ** 0.5, label=r"${\sqrt{\bar{\alpha}_t}}$")
plt.plot((1 - noise_scheduler.alphas_cumprod.cpu()) ** 0.5, label=r"$\sqrt{(1 - \bar{\alpha}_t)}$")
plt.legend(fontsize="x-large")
```

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary.svg" alt="Bref aper√ßu du contenu du cours.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary-dark.svg" alt="Bref aper√ßu des diff√©rents chapitres du cours.">
</div>

Au d√©part, le x bruit√© est principalement x (sqrt_alpha_prod ~= 1), mais au fil du temps, la contribution de x diminue et la composante bruit augmente. Contrairement √† notre m√©lange lin√©aire de x et de bruit en fonction de la quantit√©, celui-ci devient bruyant relativement rapidement. Nous pouvons visualiser cela sur quelques donn√©es :

```py
# Bruit d'un batch d'images pour visualiser l'effet
fig, axs = plt.subplots(3, 1, figsize=(16, 10))
xb, yb = next(iter(train_dataloader))
xb = xb.to(device)[:8]
xb = xb * 2. - 1. # Pour aller dans (-1, 1)
print('X shape', xb.shape)

# Afficher les entr√©es propres
axs[0].imshow(torchvision.utils.make_grid(xb[:8])[0].detach().cpu(), cmap='Greys')
axs[0].set_title('Clean X')

# Ajouter du bruit avec le planificateur
timesteps = torch.linspace(0, 999, 8).long().to(device)
noise = torch.randn_like(xb) # << NB: randn et non rand
noisy_xb = noise_scheduler.add_noise(xb, noise, timesteps)
print('Noisy X shape', noisy_xb.shape)

# Afficher la version bruyante (avec et sans coupure)
axs[1].imshow(torchvision.utils.make_grid(noisy_xb[:8])[0].detach().cpu().clip(-1, 1),  cmap='Greys')
axs[1].set_title('Noisy X (clipped to (-1, 1)')
axs[2].imshow(torchvision.utils.make_grid(noisy_xb[:8])[0].detach().cpu(),  cmap='Greys')
axs[2].set_title('Noisy X')
```

```py
X shape torch.Size([8, 1, 28, 28]) 
Noisy X shape torch.Size([8, 1, 28, 28])
```

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary.svg" alt="Bref aper√ßu du contenu du cours.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary-dark.svg" alt="Bref aper√ßu des diff√©rents chapitres du cours.">
</div>

Une autre dynamique est en jeu : la version DDPM ajoute un bruit tir√© d'une distribution gaussienne (moyenne 0, √©cart-type 1 de `torch.randn`) plut√¥t que le bruit uniforme entre 0 et 1 (de torch.rand) que nous avons utilis√© dans notre fonction corrompue d'origine. En g√©n√©ral, il est judicieux de normaliser √©galement les donn√©es d'entra√Ænement. Dans l'autre *notebook*, vous verrez `Normalize(0.5, 0.5)` dans la liste des transformations, qui fait correspondre les donn√©es de l'image de (0, 1) √† (-1, 1) et qui est "suffisante" pour nos besoins. Nous ne l'avons pas fait pour ce *notebook*, mais la cellule de visualisation ci-dessus l'ajoute pour une mise √† l'√©chelle et une visualisation plus pr√©cises.


## Objectif d'entra√Ænement

Dans notre exemple, le mod√®le tente de pr√©dire l'image d√©bruit√©e. Dans le DDPM et dans de nombreuses autres impl√©mentations de mod√®les de diffusion, le mod√®le pr√©dit le bruit utilis√© dans le processus de corruption (avant la mise √† l'√©chelle, donc un bruit √† variance unitaire). Dans le code, cela ressemble √† quelque chose comme :

```py
noise = torch.randn_like(xb) # << NB: randn et non rand
noisy_x = noise_scheduler.add_noise(x, noise, timesteps)
model_prediction = model(noisy_x, timesteps).sample
loss = mse_loss(model_prediction, noise) # le bruit comme cible
```

Vous pouvez penser que pr√©dire le bruit (√† partir duquel nous pouvons d√©duire √† quoi ressemble l'image d√©bruit√©e) est √©quivalent √† pr√©dire directement l'image d√©bruit√©e. Alors pourquoi privil√©gier l'une plut√¥t que l'autre : est-ce simplement pour des raisons de commodit√© math√©matique ?

Il s'av√®re qu'il existe une autre subtilit√©. Nous calculons la perte √† diff√©rents moments (choisis au hasard) au cours de l'entra√Ænement. Ces diff√©rents objectifs conduiront √† une "pond√©ration implicite" diff√©rente de ces pertes, o√π la pr√©diction du bruit donne plus de poids aux niveaux de bruit plus faibles. Vous pouvez choisir des objectifs plus complexes pour modifier cette "pond√©ration implicite des pertes". Vous pouvez aussi choisir un calendrier de bruit qui donnera plus d'exemples √† un niveau de bruit plus √©lev√©. Vous pouvez demander au mod√®le de pr√©dire une "vitesse" v, que nous d√©finissons comme une combinaison de l'image et du bruit d√©pendant du niveau de bruit (voir [*Progressive Distillation for Fast Sampling of Diffusion Models*](https://arxiv.org/abs/2202.00512)). Il se peut que le mod√®le pr√©dise le bruit, mais qu'il r√©duise ensuite la perte en fonction d'un facteur d√©pendant de la quantit√© de bruit, sur la base d'un peu de th√©orie (voir [*Perception Prioritized Training of Diffusion Models*](https://arxiv.org/abs/2204.00227)) ou d'exp√©riences visant √† d√©terminer quels niveaux de bruit sont les plus informatifs pour le mod√®le (voir [*Elucidating the Design Space of Diffusion-Based Generative Models*](https://arxiv.org/abs/2206.00364)). En r√©sum√© : le choix de l'objectif a un effet sur les performances du mod√®le, et des recherches sont en cours pour d√©terminer la "meilleure" option.

Pour l'instant, la pr√©diction du bruit (epsilon ou eps) est l'approche privil√©gi√©e, mais avec le temps, nous verrons probablement d'autres objectifs pris en charge dans la biblioth√®que et utilis√©s dans diff√©rentes situations.


## Conditionnement du pas de temps

Le mod√®le UNet2DModel prend en compte √† la fois x et le pas de temps. Ce dernier est transform√© en int√©gration et introduit dans le mod√®le √† plusieurs endroits.

La th√©orie sous-jacente est qu'en donnant au mod√®le des informations sur le niveau de bruit, il peut mieux accomplir sa t√¢che. Bien qu'il soit possible d'entra√Æner un mod√®le sans ce conditionnement du pas de temps, cela semble am√©liorer les performances dans certains cas et la plupart des impl√©mentations l'incluent, du moins dans la litt√©rature actuelle.

## √âchantillonnage

√âtant donn√© un mod√®le qui estime le bruit pr√©sent dans une entr√©e bruyante (ou qui pr√©dit la version d√©bruit√©e), comment produire de nouvelles images ?

Nous pourrions introduire du bruit pur et esp√©rer que le mod√®le pr√©dise une bonne image en tant que version d√©bruit√©e en une seule √©tape. Cependant, comme nous l'avons vu dans les exp√©riences ci-dessus, cela ne fonctionne g√©n√©ralement pas bien. C'est pourquoi nous proc√©dons √† un certain nombre de petites √©tapes bas√©es sur la pr√©diction du mod√®le, en √©liminant de mani√®re it√©rative une petite partie du bruit √† la fois.

La mani√®re exacte de proc√©der d√©pend de la m√©thode d'√©chantillonnage utilis√©e. Nous n'entrerons pas dans la th√©orie trop profond√©ment, mais les questions cl√©s de la conception sont les suivantes :
- Quelle est l'ampleur du pas √† franchir ? En d'autres termes, quel "calendrier de bruit" devez-vous suivre ?
- Utilisez-vous uniquement la pr√©diction actuelle du mod√®le pour informer l'√©tape de mise √† jour (comme DDPM, DDIM et beaucoup d'autres) ? √âvaluez-vous le mod√®le plusieurs fois pour estimer les gradients d'ordre sup√©rieur en vue d'une √©tape plus importante et plus pr√©cise (m√©thodes d'ordre sup√©rieur et certains solveurs d'EDO discr√®tes) ? Ou bien conservez-vous un historique des pr√©dictions pass√©es pour essayer de mieux informer l'√©tape de mise √† jour actuelle (√©chantillonneurs lin√©aires multi-√©tapes et ancestraux) ?
- Ajoutez-vous du bruit suppl√©mentaire (parfois appel√© "churn") pour ajouter plus de stochasticit√© (caract√®re al√©atoire) au processus d'√©chantillonnage, ou le gardez-vous compl√®tement d√©terministe ? De nombreux √©chantillonneurs contr√¥lent ce param√®tre (tel que "eta" pour les √©chantillonneurs DDIM) afin que l'utilisateur puisse choisir.

La recherche sur les m√©thodes d'√©chantillonnage pour les mod√®les de diffusion √©volue rapidement et de plus en plus de m√©thodes permettant de trouver de bonnes solutions en moins d'√©tapes sont propos√©es. Les courageux et les curieux trouveront peut-√™tre int√©ressant de parcourir le code des diff√©rentes impl√©mentations disponibles dans la biblioth√®que ü§ó *Diffusers* [ici](https://github.com/huggingface/diffusers/tree/main/src/diffusers/schedulers) ou de consulter la [documentation](https://huggingface.co/docs/diffusers/api/schedulers) qui renvoient souvent aux articles pertinents.


## Conclusions
Nous esp√©rons que ce *notebook* vous a permis d'aborder les mod√®les de diffusion sous un angle l√©g√®rement diff√©rent.

Ce *notebook* a √©t√© √©crit pour le cours de Hugging Face par Jonathan Whitaker, et recoupe une version incluse dans son propre cours, [*The Generative Landscape*](https://johnowhitaker.github.io/tglcourse/dm1.html). Consultez-le (en anglais) si vous souhaitez voir cet exemple de base √©tendu avec du bruit et du conditionnement de classe. Les questions ou les bugs peuvent √™tre communiqu√©s via GitHub issues ou via Discord. Vous pouvez √©galement envoyer un message via Twitter √† [@johnowhitaker](https://twitter.com/johnowhitaker).